\documentclass[10pt,landscape]{article}
%\pagestyle{headings}
\usepackage{multicol}
\usepackage[landscape]{geometry} 
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{enumerate}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage{listings}
\usepackage[pdftex]{graphicx}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
%\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}
\setlength{\topmargin}{-1.35in}
\setlength{\textheight}{8in}
\setlength{\oddsidemargin}{-0.75in}
\setlength{\evensidemargin}{-0.75in}
\setlength{\textwidth}{10.5in}
%\setlength{\baselineskip}{-1pt}
%\renewcommand{\baselinestretch}{0.5}

\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

\newcommand{\ps}{\\ \vspace{0.05in}}

% Don't print section numbers
\setcounter{secnumdepth}{0}

\thispagestyle{empty} 

\begin{document}

\raggedright
\begin{multicols}{3}

\section{COSC 201 Final Review - Feynman Liang}

\begin{scriptsize}

\section{Asymptotic Analysis}
Focuses on dominant asymptotic term (generally highest order
polynomial of n), non-dominant terms annihilated during proof.

To prove, find $c$ and $n_0$ which holds for:
\begin{itemize}
\item $O(g(n))$ - $\exists c > 0$ s.t. $\forall n > n_o$, $0 \leq f(n)
  \leq c g(n)$
\item $\Theta(g(n))$ - $\exists c_1,c_2 > 0$ s.t. $\forall n > n_o$,
  $0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n)$
\item $\Omega(g(n))$ - $\exists c > 0$ s.t. $\forall n > n_o$, $0 \leq
  c g(n) \leq f(n)$
\end{itemize}

\section{Recurrence}
Function which calls itself. Needs a non-recursive \textbf{base case}.

\textbf{Pre/post conditions} - truth at start/end of method
\textbf{Assertions} - truth at point in execution
\textbf{loop invariants} - truth at each iteration of loop

\subsection{Master Theorem}
Quick way to solve recurrences

\begin{math}
  T(n) = aT(\frac{n}{b} + f(n)) \mathrm{where} a \geq 1, b > 1
\end{math}

\begin{itemize}
\item Case 1: If $f(n) \in O(n^{\log_{b}{a}-\epsilon})$ for some constant
  $\epsilon > 0$, then $T(n) \in \Theta(n^{\log_{b}{a}})$
\item Case 2: If $f(n) \in \Theta(n^{\log_{b}{a}}\log^{k}{n})$ for some constant
  $k \geq 0$, then $T(n) \in \Theta(n^{\log_{b}{a}}\log^{k+1}{n})$
\item Case 3: If $f(n) \in \Omega(n^{\log_{b}{a}+\epsilon})$ for some constant
  $\epsilon > 0$ \textbf{AND} $af(\frac{n}{b}) \leq cf(n)$ for some
  constant $c<1$ and sufficiently large n, then $T(n) \in \Theta(n^{\log_{b}{a}})$
\end{itemize}

Ex: Straussen's matrix multiplication:
\begin{math}
  f(n) =
  \begin{cases}
    1 & \mathrm{ if} n=1\\
    7f(\frac{n}{2}) + 18(\frac{n}{2})^2 & \mathrm{ if} n \not= 1
  \end{cases}
\end{math}

This recurrence is case 1 ($18(\frac{n}{2})^2 \in
O(n^{\log_{2}{7}-\epsilon})$ for $\epsilon \leq \log_{2}{7} - 2$), thus $f(n) \in O(n^{\log_{2}{7}})$

\subsection{Inductive Proof}
To prove $P(k)$ is true for $\forall k \geq b$ using strong induction:
\begin{enumerate}
\item Prove $P(b)$ for some base case b
\item Assume $P(n)$ is true for $b \leq n \leq k$, show that this
  implies $P(k+1)$
\end{enumerate}
Weak induction only assumes $P(k)$ to prove $P(k+1)$. This doesn't
allow you to use any states past the $n-1$ state while proving P(n).

\subsection{Divide and Conquer}
Can be represented by a recurrence tree with number levels
= depth of recurrence and children per node = number of sub-problems
per recurrence.

\subsection{Dynamic Programming}
Solves optimization problems which exhibit
\textbf{optimal substructure} (optimal sol'n depends on optimal
solution of subproblems). To implement, define the optimal solution to the
problem recursively (ex. best(a,b) = best(a,k) + best(k+1,b) for
k=a..b) and solve from bottom up.

Hardest part is identifying subproblem. Monotonically increasing
subseq or longest subset looks at subseqs ending at i. Pretty
print looks at ending line with word w. Seam finding looks at three
options (straight, left, right).

Examples: fill in vector (fib, pick stamps, pretty print, rod
cutting), fill row-by-row matrix (longest common subseq, seam finding)

\subsubsection{Memoization}
Cache past calculations, improve performance on overlapping
subproblems. 

\subsubsection{Solution reconstruction}
Add a step to algorithm which keeps track of how optimal value was
obtained by filling in array.

\section{Data Structures/ADTs}
Data structures are used to abstract lower level implementation
details while allowing organization of data. ADTs provide operation on
the data structures. 

\subsection{Arrays}
\begin{itemize}
\item Insert - O(1) unsorted, O(n) sorted
\item Delete - O(1) (O(n-1) if need to shift)
\item Search - O(n) unsorted, O(log(n)) sorted
\end{itemize}

\subsection{Priority Queue}
Data structure which maintains sorted order. Typically implemented
using min/max heap or sorted array.

\subsection{Min/Max Heap}
Tree structure which has property that
$\forall$children $\leq$(max heap) or $\geq$(min heap) parent. Implies
root node is max/min. To maintain heap property, must sift up (compare two
children for one parent slot).

Sorted array can be used as min/max heap where every $2^k$ entries
represent the kth level of the heap ([1,2,2,3,3,3,3,...]).

For binary (max 2 children) heap:
\begin{itemize}
\item insert - O(log n) - insert to empty on bottom and sift up
\item getMin/Max - O(1) - get root element
\item deleteMin/Max - O(log n) - delete top and sift up
\end{itemize}

\subsection{Graphs}
A graph contains \emph{n} vertices connected by \emph{m} edges. Degree
of vertex is number of edges it has, max degree is denoted
\emph{d}. Paths can have weight (weighted) and direction (digraph).

Simple path is a path which does not visit a node twice, can prove
shortest path is simple using induction on removing cycles in path.

Undirected max number of edges: 
\begin{math}
  \begin{pmatrix}
    n \\ k
  \end{pmatrix} = \frac{n(n-1)}{2}
\end{math}, directed $n^2$

\begin{tabular}{l|c|c|c}
  Type & allNeighbors & isNeighbor & insertEdge\\
  \hline
  Adj. Matrix & O(n) & O(1) & O(1)\\
  Adj. List & O(d) & O(d) & O(1)\\
  Edge List & O(m) & O(m) & O(1)\\
  Vertex-Edge & O(max(m,d)) & O(max(m,d)) & O(m+n)\\
\end{tabular}

Adj. matrix has isNeighbor run time advantage, space $O(n^2)$. Adj. list has
allNeighbors advantage, space $O(nd)$.

\subsection{Set}
Collection of elements with no redundancies. Maintaining set independence
requires looking up the element during insertion to check
redundancies. Hash table implementation $O(1)$ for all, array
implementation:

\begin{tabular}{l|c|c}
  Op & Unsorted & Sorted\\
  \hline
  insert & O(n) & O(n)\\
  lookup & O(n) & O(log n)\\
  delete & O(n/2) & O(n)
\end{tabular}

\subsection{Hash Table}
Can be used to implement map/set/dictionary. Contains \emph{M} buckets
and \emph{n} elements. Uses a hash function to convert input to a
bucket index. 

Good hash functions have uniform distribution of hash values,
computationally cheap. Typically consume set number of bits and
performs bit operations to hash. 

Without collisions, runtimes are:

\begin{itemize}
\item Insert - O(1)
\item Delete - O(1)
\item Lookup - O(1)
\end{itemize}

To resolve collisions, we can find a new bucket (\textbf{open
addressing}) using different methods (h(k,i) returns another place to
insert i if bucket h(k) is taken):

\begin{itemize}
\item Linear probing - $h(k,i) = (h(k) + i) \mod M$
\item Quadratic probing - $h(k,i) = (h(k) + i^2) \mod M$ 
\item Double hashing -  $h(k,i) = (h(k) + i*h_2(k)) \mod M$
\end{itemize}

Problem with linear probing is if something collides once, it will
always collide. Simpler probing faster but more likely to cause
\textbf{clustering} - filled blocks which increase number of
collisions. Large clusters at end of buckets effectively shrinks size
of M. 

Quadratic and double hash resolve by varying probed
bucket dependent upon i or $h_2$.

\textbf{Separate chaining} is another collision resolution method
where we just append to collided bucket (make linked list). Operations
are $\Theta(n+M)$. Performance degrades more gracefully, don't need to
resize when full.

Performance of hash map depends on \textbf{load factor} = n/m.

\section{Problems and Algorithms}

\subsection{Sorting}
\textbf{Stable} - Relative positions of elements don't change after sort\\
\textbf{In place} - Requires constant memory overhead

\subsubsection{Insertion Sort}
Construct sorted sub-array at start of array. Insert elements one by
one and swap to maintain order. $O(n^2)$

\subsubsection{Merge Sort}
Divide and conquer. Recursively split array down middle until single
elements, then merge all back together. $T(n) = 2T(\frac{n}{2}) + cn
\in \Theta(n \log n)$ 

\subsubsection{Quick Sort}
Pick partition element k, partition around k [$<k$,k,$>k$]. Recur on
both sides. Expected $O(n\log n)$, worst $O(n^2)$ if partition is
always max or min. Can randomize partition element to avoid worst case.

\subsection{Selection of kth largest}
\subsubsection{Binary Search}
Divide and conquer on sorted array. Guesses middle element and recurs on
side dependent upon whether guess was $>$ or $<$ expected. $T(n) =
T(\frac{n}{2}) + c \in \Theta(\log n)$
\subsubsection{Quick Select}
Randomized quick sort, discards half which does not contain kth largest,
$O(n^2)$ worst (always partition around min/max), $\Omega(n\log n)$ best.
\subsubsection{Median of medians}
Deterministic Quick Select. Partition into blocks of size 5, calculate
medians of the n/5 blocks and take median of that to be new partition
element. $T(\frac{n}{5}) + T(\frac{7n}{10}) + O(n) \in O(n)$ time.

\subsection{Matrix Algorithms}

\subsubsection{Matrix Multiplication}
Dynamic programming solves optimal chain parenthesis by diagonally
filling in a solution matrix.

Straussen's solves multiplication in $O(n^{\log_{2}{7}})$.

\subsubsection{Floyd-Warshall}
Solves APSP/transitive closure in $O(n^3)$ instead of $n^4$ by moving
k loop outside. Iterates over intermediate node k rather than path length.
\begin{lstlisting}
T <- G
  for (k=1 to n)
    for (i=1 to n)
      for (j=1 to n)
	//Transitive Closure (Warshall algorithm)
	T[i,j] = T[i,j] || (T[i,k] && T[k,j])
	//All Pairs Shortest Paths (Floyd's algorithm)
        //iterate across the ks (all table lookups)
	M[i,j] = minof(T[i,j], T[i,k] + T[k,j])
\end{lstlisting}
\end{scriptsize}
\end{multicols}
\end{document}